{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil, time, os, requests, random, copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 16):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "\n",
    "!tar -xf /content/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "  \n",
    "train_files = ['data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5']\n",
    "images = np.array([],dtype=np.uint8).reshape((0,3072))\n",
    "labels = np.array([])\n",
    "for tf in train_files:\n",
    "    data_dict = unpickle('/content/cifar-10-batches-py/'+tf)\n",
    "    data = data_dict[b'data']\n",
    "    images = np.append(images,data,axis=0)\n",
    "    labels = np.append(labels,data_dict[b'labels'])\n",
    "#print(images.shape, labels.shape)\n",
    "\n",
    "testimages = np.array([],dtype=np.uint8).reshape((0,3072))\n",
    "testlabels = np.array([])\n",
    "\n",
    "data_dict = unpickle('/content/cifar-10-batches-py/test_batch')\n",
    "data = data_dict[b'data']\n",
    "testimages = np.append(testimages,data,axis=0)\n",
    "testlabels = np.append(testlabels,data_dict[b'labels'])\n",
    "#print(testimages.shape, testlabels.shape)\n",
    "\n",
    "images = images.reshape((-1,3,32,32)).astype(np.float)\n",
    "testimages = testimages.reshape((-1,3,32,32)).astype(np.float)\n",
    "\n",
    "lab_dict = {0:'airplane',1:'automobile',2:'bird',3:'cat',4:'deer',5:'dog',6:'frog',7:'horse',8:'ship',9:'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimages = images[:40000]\n",
    "valimages = images[40000:]\n",
    "trlabels = labels[:40000]\n",
    "vallabels = labels[40000:]\n",
    "MEAN = np.mean(trimages/255.0,axis=(0,2,3),keepdims=True)\n",
    "STD = np.std(trimages/255.0,axis=(0,2,3),keepdims=True)\n",
    "print(MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = C10DataGen('train',trimages)\n",
    "dl = DataLoader(dg,batch_size = 128,drop_last=True)\n",
    "\n",
    "vdg = C10DataGen('valid',valimages)\n",
    "vdl = DataLoader(vdg,batch_size = 128,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ad3c23498d49659b72017b285cfe79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 use_bias = True,\n",
    "                 use_bn = False,\n",
    "                 **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_features, \n",
    "                                self.out_features, \n",
    "                                bias = self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "             self.bn = nn.BatchNorm1d(self.out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 head_type = 'nonlinear',\n",
    "                 **kwargs):\n",
    "        super(ProjectionHead,self).__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.head_type = head_type\n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features,self.out_features,False, True)\n",
    "        elif self.head_type == 'nonlinear':\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features,self.hidden_features,True, True),\n",
    "                nn.ReLU(),\n",
    "                LinearLayer(self.hidden_features,self.out_features,False,True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class PreModel(nn.Module):\n",
    "    def __init__(self,base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        #PRETRAINED MODEL\n",
    "        self.pretrained = models.resnet50(pretrained=True)\n",
    "        \n",
    "        self.pretrained.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
    "        self.pretrained.maxpool = Identity()\n",
    "        \n",
    "        self.pretrained.fc = Identity()\n",
    "        \n",
    "        for p in self.pretrained.parameters():\n",
    "            p.requires_grad = True\n",
    "        \n",
    "        self.projector = ProjectionHead(2048, 2048, 128)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.pretrained(x)\n",
    "        \n",
    "        xp = self.projector(torch.squeeze(out))\n",
    "        \n",
    "        return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PreModel('resnet50').to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super(SimCLR_Loss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "\n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "        \n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import re\n",
    "\n",
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    \"\"\"\n",
    "    Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "    Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=required,\n",
    "        momentum=0.9,\n",
    "        use_nesterov=False,\n",
    "        weight_decay=0.0,\n",
    "        exclude_from_weight_decay=None,\n",
    "        exclude_from_layer_adaptation=None,\n",
    "        classic_momentum=True,\n",
    "        eeta=EETA_DEFAULT,\n",
    "    ):\n",
    "        \"\"\"Constructs a LARSOptimizer.\n",
    "        Args:\n",
    "        lr: A `float` for learning rate.\n",
    "        momentum: A `float` for momentum.\n",
    "        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "        weight_decay: A `float` for weight decay.\n",
    "        exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "            any of the string appears in a variable's name, the variable will be\n",
    "            excluded for computing weight decay. For example, one could specify\n",
    "            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "            from weight decay.\n",
    "        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "            for layer adaptation. If it is None, it will be defaulted the same as\n",
    "            exclude_from_weight_decay.\n",
    "        classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "            momentum. The learning rate is applied during momeuntum update in\n",
    "            classic momentum, but after momentum for popular momentum.\n",
    "        eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "        name: The name for the scope.\n",
    "        \"\"\"\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            use_nesterov=use_nesterov,\n",
    "            weight_decay=weight_decay,\n",
    "            exclude_from_weight_decay=exclude_from_weight_decay,\n",
    "            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n",
    "            classic_momentum=classic_momentum,\n",
    "            eeta=eeta,\n",
    "        )\n",
    "\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_nesterov = use_nesterov\n",
    "        self.classic_momentum = classic_momentum\n",
    "        self.eeta = eeta\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "        # arg is None.\n",
    "        if exclude_from_layer_adaptation:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "        else:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            eeta = group[\"eeta\"]\n",
    "            lr = group[\"lr\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param = p.data\n",
    "                grad = p.grad.data\n",
    "\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # TODO: get param names\n",
    "                # if self._use_weight_decay(param_name):\n",
    "                grad += self.weight_decay * param\n",
    "\n",
    "                if self.classic_momentum:\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                    # TODO: get param names\n",
    "                    # if self._do_layer_adaptation(param_name):\n",
    "                    w_norm = torch.norm(param)\n",
    "                    g_norm = torch.norm(grad)\n",
    "\n",
    "                    device = g_norm.get_device()\n",
    "                    trust_ratio = torch.where(\n",
    "                        w_norm.gt(0),\n",
    "                        torch.where(\n",
    "                            g_norm.gt(0),\n",
    "                            (self.eeta * w_norm / g_norm),\n",
    "                            torch.Tensor([1.0]).to(device),\n",
    "                        ),\n",
    "                        torch.Tensor([1.0]).to(device),\n",
    "                    ).item()\n",
    "\n",
    "                    scaled_lr = lr * trust_ratio\n",
    "                    if \"momentum_buffer\" not in param_state:\n",
    "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
    "                            p.data\n",
    "                        )\n",
    "                    else:\n",
    "                        next_v = param_state[\"momentum_buffer\"]\n",
    "\n",
    "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
    "                    if self.use_nesterov:\n",
    "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
    "                    else:\n",
    "                        update = next_v\n",
    "\n",
    "                    p.data.add_(-update)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _use_weight_decay(self, param_name):\n",
    "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "        if not self.weight_decay:\n",
    "            return False\n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def _do_layer_adaptation(self, param_name):\n",
    "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "        if self.exclude_from_layer_adaptation:\n",
    "            for r in self.exclude_from_layer_adaptation:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Epoch     0: adjusting learning rate of group 0 to 2.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "#OPTMIZER\n",
    "optimizer = LARS(\n",
    "    [params for params in model.parameters() if params.requires_grad],\n",
    "    lr=0.2,\n",
    "    weight_decay=1e-6,\n",
    "    exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    ")\n",
    "\n",
    "# \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "#SCHEDULER OR LINEAR EWARMUP\n",
    "warmupscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch : (epoch+1)/10.0, verbose = True)\n",
    "\n",
    "#SCHEDULER FOR COSINE DECAY\n",
    "mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 500, eta_min=0.05, last_epoch=-1, verbose = True)\n",
    "\n",
    "#LOSS FUNCTION\n",
    "criterion = SimCLR_Loss(batch_size = 128, temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, current_epoch, name):\n",
    "    out = os.path.join('./cifar/content/saved_models/',name.format(current_epoch))\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict()}, out)\n",
    "\n",
    "def plot_features(model, num_classes, num_feats, batch_size):\n",
    "    preds = np.array([]).reshape((0,1))\n",
    "    gt = np.array([]).reshape((0,1))\n",
    "    feats = np.array([]).reshape((0,num_feats))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x1,x2 in vdl:\n",
    "            x1 = x1.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
    "            out = model(x1)\n",
    "            out = out.cpu().data.numpy()#.reshape((1,-1))\n",
    "            feats = np.append(feats,out,axis = 0)\n",
    "    \n",
    "    tsne = TSNE(n_components = 2, perplexity = 50)\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "    num_samples = int(batch_size*(valimages.shape[0]//batch_size))#(len(val_df)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        plt.scatter(x_feats[vallabels[:num_samples]==i,1],x_feats[vallabels[:num_samples]==i,0])\n",
    "    \n",
    "    plt.legend([str(i) for i in range(num_classes)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, current_epoch, name):\n",
    "    out = os.path.join('./cifar/content/saved_models/',name.format(current_epoch))\n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict()}, out)\n",
    "\n",
    "def plot_features(model, num_classes, num_feats, batch_size):\n",
    "    preds = np.array([]).reshape((0,1))\n",
    "    gt = np.array([]).reshape((0,1))\n",
    "    feats = np.array([]).reshape((0,num_feats))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x1,x2 in vdl:\n",
    "            x1 = x1.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
    "            out = model(x1)\n",
    "            out = out.cpu().data.numpy()#.reshape((1,-1))\n",
    "            feats = np.append(feats,out,axis = 0)\n",
    "    \n",
    "    tsne = TSNE(n_components = 2, perplexity = 50)\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "    num_samples = int(batch_size*(valimages.shape[0]//batch_size))#(len(val_df)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        plt.scatter(x_feats[vallabels[:num_samples]==i,1],x_feats[vallabels[:num_samples]==i,0])\n",
    "    \n",
    "    plt.legend([str(i) for i in range(num_classes)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Downstream model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSModel(nn.Module):\n",
    "    def __init__(self,premodel,num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.premodel = premodel\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        for p in self.premodel.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        for p in self.premodel.projector.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.lastlayer = nn.Linear(2048,self.num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.premodel.pretrained(x)\n",
    "        out = self.lastlayer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator for the Downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trimages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1751/1502954543.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDSDataGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trimages' is not defined"
     ]
    }
   ],
   "source": [
    "class DSDataGen(Dataset):\n",
    "    def __init__(self, phase, imgarr,labels,num_classes):\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.num_classes = num_classes\n",
    "        self.imgarr = imgarr\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.randomcrop = transforms.RandomResizedCrop(32,(0.8,1.0))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.imgarr.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.imgarr[idx]\n",
    "        \n",
    "        img = torch.from_numpy(x).float()\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            img  = self.randomcrop(img)\n",
    "\n",
    "        img = self.preprocess(img)\n",
    "        \n",
    "        return img, label\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        idx = random.sample(population = list(range(self.__len__())),k = self.__len__())\n",
    "        self.imgarr = self.imgarr[idx]\n",
    "        self.labels = self.labels[idx]\n",
    "        \n",
    "    def preprocess(self,frame):\n",
    "        frame = frame / 255.0\n",
    "        frame = (frame-MEAN)/STD\n",
    "        return frame\n",
    "\n",
    "dg = DSDataGen('train', trimages, trlabels, num_classes=10)\n",
    "\n",
    "dl = DataLoader(dg,batch_size = 32, drop_last = True)\n",
    "\n",
    "vdg = DSDataGen('valid', valimages, vallabels, num_classes=10)\n",
    "\n",
    "vdl = DataLoader(vdg,batch_size = 32, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1751/329838181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_cl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdsoptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsoptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.98\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dsmodel' is not defined"
     ]
    }
   ],
   "source": [
    "# Some more declarations\n",
    "tr_ep_loss = []\n",
    "tr_ep_acc = []\n",
    "\n",
    "val_ep_loss = []\n",
    "val_ep_acc = []\n",
    "\n",
    "min_val_loss = 100.0\n",
    "\n",
    "EPOCHS = 10\n",
    "num_cl = 10\n",
    "dsmodel = DSModel\n",
    "dsoptimizer = torch.optim.SGD([params for params in dsmodel.parameters() if params.requires_grad],lr = 0.01, momentum = 0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(dsoptimizer, step_size=1, gamma=0.98, last_epoch=-1, verbose = True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    \n",
    "    stime = time.time()\n",
    "    print(\"=============== Epoch : %3d ===============\"%(epoch+1))\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    #iter_num = 0\n",
    "    dsmodel.train()\n",
    "    \n",
    "    dsoptimizer.zero_grad()\n",
    "    \n",
    "    for x,y in dl:\n",
    "        x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
    "        y = y.to(device = 'cuda:0')\n",
    "        \n",
    "        z = dsmodel(x)\n",
    "        \n",
    "        dsoptimizer.zero_grad()\n",
    "        \n",
    "        tr_loss = loss_fn(z,y)\n",
    "        tr_loss.backward()\n",
    "\n",
    "        preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "        \n",
    "        dsoptimizer.step()\n",
    "        \n",
    "        loss_sublist = np.append(loss_sublist, tr_loss.cpu().data)\n",
    "        acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "        \n",
    "    print('ESTIMATING TRAINING METRICS.............')\n",
    "    \n",
    "    print('TRAINING BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "    print('TRAINING BINARY ACCURACY: ',np.mean(acc_sublist))\n",
    "    \n",
    "    tr_ep_loss.append(np.mean(loss_sublist))\n",
    "    tr_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    print('ESTIMATING VALIDATION METRICS.............')\n",
    "    \n",
    "    dsmodel.eval()\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in vdl:\n",
    "            x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
    "            y = y.to(device = 'cuda:0')\n",
    "            z = dsmodel(x)\n",
    "\n",
    "            val_loss = loss_fn(z,y)\n",
    "\n",
    "            preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "\n",
    "            loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n",
    "            acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "    \n",
    "    print('VALIDATION BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "    print('VALIDATION BINARY ACCURACY: ',np.mean(acc_sublist))\n",
    "    \n",
    "    val_ep_loss.append(np.mean(loss_sublist))\n",
    "    val_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    dg.on_epoch_end()\n",
    "    \n",
    "    if np.mean(loss_sublist) <= min_val_loss:\n",
    "        min_val_loss = np.mean(loss_sublist) \n",
    "        print('Saving model...')\n",
    "        torch.save({'model_state_dict': dsmodel.state_dict(),\n",
    "                'optimizer_state_dict': dsoptimizer.state_dict()}, \n",
    "               '/content/saved_models/cifar10_rn50_p128_sgd0p01_decay0p98_all_lincls.pt')\n",
    "    \n",
    "    print(\"Time Taken : %.2f minutes\"%((time.time()-stime)/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdg = DSDataGen('test', testimages, testlabels, num_classes=10)\n",
    "\n",
    "tdl = DataLoader(tdg, batch_size = 32, drop_last = True)\n",
    "\n",
    "dsmodel.eval()\n",
    "    \n",
    "loss_sublist = np.array([])\n",
    "acc_sublist = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x,y in vdl:\n",
    "        x = x.squeeze().to(device = 'cuda:0', dtype = torch.float)\n",
    "        y = y.to(device = 'cuda:0')\n",
    "        z = dsmodel(x)\n",
    "\n",
    "        val_loss = loss_fn(z,y)\n",
    "\n",
    "        preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "\n",
    "        loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n",
    "        acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "\n",
    "print('TEST BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "print('TEST BINARY ACCURACY: ',np.mean(acc_sublist))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f44a392e09d1bb9477de132905dd7ad0f7bbaeeffcb9fda9d53bc480fa8090eb"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
